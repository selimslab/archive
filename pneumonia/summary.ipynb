{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import keras\n",
    "import random \n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96e1c06d86457d771503de149f682099b8916781"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import pydicom \n",
    "\n",
    "data_path = '../input/rsna-pneumonia-detection-challenge/'\n",
    "\n",
    "images_path = data_path + 'stage_2_train_images/'\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,\n",
    "                 list_IDs, \n",
    "                 labels, \n",
    "                 batch_size=32,\n",
    "                 dim=(32,32,32), \n",
    "                 n_channels=1,\n",
    "                 n_classes=10,\n",
    "                 shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def read_image_from_dicom(self,ID):\n",
    "        dicom_path = images_path + ID + '.dcm'\n",
    "        img = pydicom.dcmread(dicom_path).pixel_array\n",
    "        img = np.array(img)\n",
    "        img = np.resize(img, self.dim)\n",
    "        img = np.stack((img,)*3, axis=-1) \n",
    "        return img\n",
    "        \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(tqdm(list_IDs_temp)):\n",
    "            # Store sample\n",
    "            img = self.read_image_from_dicom(ID)\n",
    "            X[i,] = img\n",
    "            # Store class\n",
    "            y[i] = self.labels[i]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2afb267eabaf308067a78277508b510798420b4"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def read_pickle(path):    \n",
    "    with open(path, 'rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "    \n",
    "data_path = '../input/samwise/'\n",
    "labels_path = data_path + 'labels'\n",
    "patient_ids_path = data_path + 'ids'\n",
    "\n",
    "labels = read_pickle(labels_path)\n",
    "ids = read_pickle(patient_ids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature vectors from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "# load only conv layers, with global average pooling \n",
    "base_model = DenseNet121(include_top=False, weights='imagenet', pooling='avg')\n",
    "\"\"\"\n",
    "xception \n",
    "vgg16\n",
    "vgg19\n",
    "inception\n",
    "densenet201\n",
    "mobilenetv2\n",
    "resnet50\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca6e1c31289962f38208a6b652b82c6ce81ca587"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (224,224), # default input size for VGG16\n",
    "          'batch_size': 32,\n",
    "          'n_classes': 3,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': False}\n",
    "\n",
    "feature_generator = DataGenerator(ids,labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71df7a006be8bed427a038cb92f2f5ae8736c9d8"
   },
   "outputs": [],
   "source": [
    "feature_tensors = base_model.predict_generator(feature_generator, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64956dc7fff15778895c61b7707bf4ba7889bb1e"
   },
   "outputs": [],
   "source": [
    "np.save('feature_vectors', feature_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0794fbb63cc5b0168b667dae82e07ddcbba24a1c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import pydicom\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "def read_pickle(filename):    \n",
    "    with open(filename, 'rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "    \n",
    "def save_pickle(data,filename):       \n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(data, fp)  \n",
    "    \n",
    "data_path = '../input/' # rsna-pneumonia-detection-challenge/\n",
    "\n",
    "images_path = data_path + 'stage_2_train_images/'\n",
    "labels_path = data_path + 'stage_2_train_labels.csv'\n",
    "\n",
    "detailed_class_info_path = data_path + 'stage_2_detailed_class_info.csv'\n",
    "\n",
    "class_encoder = LabelEncoder()\n",
    "   \n",
    "def merge_dataframes():\n",
    "    df = pd.read_csv(labels_path)\n",
    "    details_df = pd.read_csv(detailed_class_info_path)\n",
    "    df = pd.concat([df,details_df.drop('patientId',1)], 1) \n",
    "    print(df.describe())\n",
    "    print(df.shape[0], 'cases')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "  \n",
    "def load_ids_and_labels_from_file():\n",
    "    ids = read_pickle('ids')\n",
    "    labels = read_pickle('labels')\n",
    "    return ids,labels\n",
    "  \n",
    "def get_ids_and_labels(num_class):\n",
    "    df = merge_dataframes()\n",
    "    df['class_id'] = class_encoder.fit_transform(df['class'])\n",
    "\n",
    "    df.sort_values(by=['patientId', 'class_id'])\n",
    "    \n",
    "    ids = df.patientId.tolist()\n",
    "    \n",
    "    if(num_class==2):\n",
    "        labels =  df.Target.tolist() \n",
    "    else:\n",
    "        labels =  df.class_id.tolist() \n",
    "    \n",
    "    save_pickle(ids, 'ids')\n",
    "    save_pickle(labels, 'labels')\n",
    "\n",
    "    return ids,labels\n",
    "\n",
    "      \n",
    "\n",
    "num_class = 2\n",
    "ids,labels = get_ids_and_labels(num_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_tensors = np.load('vgg16_features.npz')['arr_0']\n",
    "\n",
    "y = labels[:feature_tensors.shape[0]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_tensors, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def precision_recall(name, clf):  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    if(num_class==2):\n",
    "        roc_score = roc_auc_score(y_test, y_pred)\n",
    "        print('roc_auc_score', roc_score)\n",
    "\n",
    "    \n",
    "    if(num_class==3):\n",
    "        report = classification_report(y_test, y_pred, target_names=class_encoder.classes_)\n",
    "    else:\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        \n",
    "    print('classification report for', name)\n",
    "    print( report )\n",
    "    \n",
    "\n",
    "def evaluate_classifier(clf,name):    \n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    save_pickle(clf,name)\n",
    "\n",
    "    precision_recall(name, clf)\n",
    "    \n",
    "    score = clf.score(X_test,y_test)\n",
    "    \n",
    "    print('average_score', round(score,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(strategy='stratified', random_state=0)\n",
    "\n",
    "name = 'Dummy stratified'\n",
    "\n",
    "evaluate_classifier(clf,name)\n",
    "\n",
    "\n",
    "\n",
    "clf = DummyClassifier(strategy='uniform', random_state=0)\n",
    "\n",
    "name = 'Dummy uniform'\n",
    "\n",
    "evaluate_classifier(clf,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "name = 'DecisionTreeClassifier'\n",
    "\n",
    "evaluate_classifier(clf,name) \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "name = 'RandomForestClassifier'\n",
    "\n",
    "evaluate_classifier(clf,name) \n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "name = 'LinearDiscriminantAnalysis'\n",
    "\n",
    "evaluate_classifier(clf,name) \n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "name = 'QuadraticDiscriminantAnalysis'\n",
    "\n",
    "evaluate_classifier(clf,name) \n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "name = 'LinearSVC'\n",
    "\n",
    "evaluate_classifier(clf,name) \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "name = 'GaussianNB'\n",
    "\n",
    "evaluate_classifier(clf,name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Flatten,BatchNormalization,LeakyReLU\n",
    "from keras.metrics import categorical_accuracy, binary_accuracy\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "if(num_class==2):\n",
    "    y = labels[:feature_tensors.shape[0]]\n",
    "    y = np.array(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_tensors, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=[binary_accuracy])\n",
    "\n",
    "else:\n",
    "    y = labels[:feature_tensors.shape[0]]\n",
    "    y = keras.utils.to_categorical(y, num_class)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_tensors, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[categorical_accuracy])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=12,\n",
    "          batch_size=16,\n",
    "          validation_data=(X_test, y_test)\n",
    "         )\n",
    "\n",
    "model.save('mobilenet.h5') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
